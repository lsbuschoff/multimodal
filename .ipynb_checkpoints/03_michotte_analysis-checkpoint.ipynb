{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyreadr\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau, fisher_exact, ttest_ind, sem, binom\n",
    "from helpers import plot_params\n",
    "from natsort import natsorted, natsort_keygen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Human data from Gerstenberg\n",
    "humans = pyreadr.read_r('data/trackingDataFrames.RData')['df.judgments']\n",
    "counterfactual = humans[humans.condition == \"counterfactual\"]\n",
    "outcome = humans[humans.condition == \"outcome\"]\n",
    "outcome.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model data we collected\n",
    "models = pd.read_csv(\"data/MICHOTTE_RESULTS_REV2.csv\")\n",
    "models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Analyze match to ground truth for color test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subjects\n",
    "subjects = [\"FUYU\", \"OTTER\", \"ADAPTER\", \"GPT-4V\", \"CLAUDE-3\"]\n",
    "colors = np.zeros(len(subjects))\n",
    "colors_errs = np.zeros(len(subjects))\n",
    "\n",
    "# Loop through subjects\n",
    "for ind, subject in enumerate(subjects):  \n",
    "\n",
    "        # Absolute distance to ground truth and absolute distance to human average\n",
    "        correct = sum(models[models.COND == \"color\"][subject] == \"white\")\n",
    "        colors[ind] = (correct / 18)*100\n",
    "        colors_errs[ind] = binom.std(100, correct / 18)\n",
    "        print(f\"Color, {subject}: {colors[ind]:.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyze visual understanding test with trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = np.zeros(len(subjects))\n",
    "trajectory_errs = np.zeros(len(subjects))\n",
    "\n",
    "# Loop through subjects\n",
    "for ind, subject in enumerate(subjects):  \n",
    "\n",
    "        # Absolute distance to ground truth and absolute distance to human average\n",
    "        correct = sum(models[models.COND == \"trajectory\"][subject] == \"right to left\")\n",
    "        trajectory[ind] = (correct / 18)*100\n",
    "        trajectory_errs[ind] = binom.std(100, correct / 18)\n",
    "        print(f\"Trajectory, {subject}: {trajectory[ind]:.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyze correlation with human data for outcome condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save human answers next to model answers for all models\n",
    "# Output shape here is: (Number of humans * Number of test images, 3) \n",
    "# With 3 being: Model answers repeated number of humans times, All human answers, Human index\n",
    "\n",
    "num_humans = len(outcome.participant.unique())\n",
    "num_seqs = 18\n",
    "human_mean = outcome[outcome[\"clip\"] < 19].groupby('clip').mean('rating')['rating'].values\n",
    "\n",
    "for model in (subjects + [\"HUMAN\"]):\n",
    "\n",
    "    # Loop through all human subjects\n",
    "    temp = np.zeros((num_humans*num_seqs, 3))\n",
    "    temp[:] = np.nan\n",
    "    for ind, human in enumerate(outcome.participant.unique()):\n",
    "\n",
    "        # If array for human to human comparison, save human mean. Otherwise, save model answers in first column.\n",
    "        if model == \"HUMAN\":\n",
    "            temp[ind*num_seqs:(ind+1)*num_seqs, 0] = human_mean\n",
    "        else:\n",
    "            temp[ind*num_seqs:(ind+1)*num_seqs, 0] = models[models.COND == \"outcome\"][model].values.astype(float)\n",
    "\n",
    "        # Save individual human answers and individual human id\n",
    "        ind_human = outcome[(outcome[\"participant\"] == human) & (outcome[\"clip\"] < 19)].sort_values(\"clip\", key=natsort_keygen())\n",
    "        temp[ind*num_seqs:(ind+1)*num_seqs, 1] = ind_human.rating.values.astype(float)\n",
    "        temp[ind*num_seqs:(ind+1)*num_seqs, 2] = human\n",
    "        \n",
    "    print(f\"{model}, {spearmanr(temp[:, 0], temp[:, 1], nan_policy=\"omit\").statistic:.4f}\")\n",
    "\n",
    "    # Save to read in R after\n",
    "    temp = pd.DataFrame(temp, columns=[\"Model\", \"Human\", \"Ind\"])\n",
    "    temp.to_csv(f\"dataframes/03_michotte/{model}_outcome.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "outcome_lmer = pd.read_csv('dataframes/03_michotte/03_outcome_lmer.csv')\n",
    "\n",
    "# Separate coefficient and re-format CI\n",
    "outcome_lmer_coef = outcome_lmer.iloc[:,0].values\n",
    "outcome_lmer_conf = outcome_lmer.iloc[:,1:].values.T\n",
    "#outcome_lmer_conf = [(outcome_lmer.iloc[:,0] - outcome_lmer.iloc[:,1]).values,\n",
    "#                     (outcome_lmer.iloc[:,2] - outcome_lmer.iloc[:,0]).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze correlation with human data for counterfactual condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save human answers next to model answers for all models\n",
    "# Output shape here is: (Number of humans * Number of test images, 3) \n",
    "# With 3 being: Model answers repeated number of humans times, All human answers, Human index\n",
    "\n",
    "num_humans = len(counterfactual.participant.unique())\n",
    "num_seqs = 18\n",
    "human_mean = counterfactual[counterfactual[\"clip\"] < 19].groupby('clip').mean('rating')['rating'].values\n",
    "\n",
    "for model in (subjects + [\"HUMAN\"]):\n",
    "\n",
    "    # Loop through all human subjects\n",
    "    temp = np.zeros((num_humans*num_seqs, 3))\n",
    "    temp[:] = np.nan\n",
    "    for ind, human in enumerate(counterfactual.participant.unique()):\n",
    "\n",
    "        # If array for human to human comparison, save human mean. Otherwise, save model answers in first column.\n",
    "        if model == \"HUMAN\":\n",
    "            temp[ind*num_seqs:(ind+1)*num_seqs, 0] = human_mean\n",
    "        else:\n",
    "            temp[ind*num_seqs:(ind+1)*num_seqs, 0] = models[models.COND == \"counterfactual\"][model].values.astype(float)\n",
    "\n",
    "        # Save individual human answers and individual human id\n",
    "        ind_human = counterfactual[(counterfactual[\"participant\"] == human) & (counterfactual[\"clip\"] < 19)].sort_values(\"clip\", key=natsort_keygen())\n",
    "        temp[ind*num_seqs:(ind+1)*num_seqs, 1] = ind_human.rating.values.astype(float)\n",
    "        temp[ind*num_seqs:(ind+1)*num_seqs, 2] = human\n",
    "        \n",
    "    print(f\"{model}, {spearmanr(temp[:, 0], temp[:, 1], nan_policy=\"omit\").statistic:.4f}\")\n",
    "\n",
    "    # Save to read in R after\n",
    "    temp = pd.DataFrame(temp, columns=[\"Model\", \"Human\", \"Ind\"])\n",
    "    temp.to_csv(f\"dataframes/03_michotte/{model}_counterfactual.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "counterfactual_lmer = pd.read_csv('dataframes/03_michotte/03_counterfactual_lmer.csv')\n",
    "\n",
    "# Separate coefficient and re-format CI\n",
    "counterfactual_lmer_coef = counterfactual_lmer.iloc[:,0].values\n",
    "counterfactual_lmer_conf = counterfactual_lmer.iloc[:,1:].values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make main plot\n",
    "I imagine a plot with 4 panels, single row with four columns,  for every study\n",
    "- A: Barplots model comparison for background color task. Bars show percentage correct.\n",
    "- B: Counting model comparison as in A\n",
    "- C: Reasoning ground truth comparison as in A\n",
    "- D: Match to humans,  probably via regression coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init plotting params\n",
    "model_colors = plot_params()\n",
    "\n",
    "# Prepare plot\n",
    "num_subjects = len(subjects)\n",
    "fig, axs = plt.subplots(1, 4, figsize=(8,3), sharey=False, tight_layout=True)  \n",
    "\n",
    "# Plot A\n",
    "axs[0].bar(np.arange(num_subjects), colors, color=model_colors)\n",
    "axs[0].errorbar(np.arange(num_subjects), colors, colors_errs, ls=\"none\", ecolor=\"gray\")\n",
    "axs[0].set_title(\"Color\\n\")\n",
    "axs[0].set_title(\"A\\n\", fontweight='bold', loc='left', fontsize='medium')\n",
    "axs[0].set_ylabel(\"Percentage correct\")\n",
    "axs[0].set_ylim(0, 100)\n",
    "#axs[0].axhline(y=random_dists[0])\n",
    "axs[0].set_xticks(np.arange(num_subjects))\n",
    "\n",
    "# Plot B\n",
    "axs[1].bar(np.arange(num_subjects), trajectory, color=model_colors)\n",
    "axs[1].errorbar(np.arange(num_subjects), trajectory, trajectory_errs, ls=\"none\", ecolor=\"gray\")\n",
    "axs[1].set_title(\"Trajectory\\n\")\n",
    "axs[1].set_title(\"B\\n\", fontweight='bold', loc='left', fontsize='medium')\n",
    "axs[1].set_ylabel(\"Percentage correct\")\n",
    "axs[1].set_ylim(0, 100)\n",
    "axs[1].axhline(50, linewidth=1, zorder=0)\n",
    "axs[1].set_xticks(np.arange(num_subjects))\n",
    "\n",
    "# Plot C\n",
    "axs[2].bar(np.arange(num_subjects+1), outcome_lmer_coef, color=model_colors)\n",
    "axs[2].errorbar(np.arange(num_subjects+1), outcome_lmer_coef, yerr=outcome_lmer_conf, ls=\"none\", ecolor=\"gray\")\n",
    "axs[2].set_title(\"Outcome\\n\", loc=\"center\")\n",
    "axs[2].set_title(\"C\\n\", fontweight='bold', loc='left', fontsize='medium')\n",
    "axs[2].set_ylabel(\"Regression to humans\")\n",
    "axs[2].set_ylim(-1, 1)\n",
    "axs[2].set_xticks(np.arange(num_subjects+1))\n",
    "\n",
    "# Plot D\n",
    "axs[3].bar(np.arange(num_subjects+1), counterfactual_lmer_coef, color=model_colors)\n",
    "axs[3].errorbar(np.arange(num_subjects+1), counterfactual_lmer_coef, yerr=counterfactual_lmer_conf, ls=\"none\", ecolor=\"gray\")\n",
    "axs[3].set_title(\"Counterfactual\\n\", loc=\"center\")\n",
    "axs[3].set_title(\"D\\n\", fontweight='bold', loc='left', fontsize='medium')\n",
    "axs[3].set_ylabel(\"Regression to humans\")\n",
    "axs[3].set_ylim(-1, 1)\n",
    "axs[3].set_xticks(np.arange(num_subjects+1))\n",
    "\n",
    "# Despine and set x tick labels afterwards\n",
    "sns.despine(offset=5, trim=True)\n",
    "axs[0].set_xticklabels(subjects, rotation=90)\n",
    "axs[1].set_xticklabels(subjects, rotation=90)\n",
    "axs[2].set_xticklabels(subjects + [\"Humans\"], rotation=90)\n",
    "axs[3].set_xticklabels(subjects + [\"Humans\"], rotation=90)\n",
    "plt.savefig(\"figures/03_michotte_main.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional: example plot for counterfactual condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print images in order from easy to hard for humans \n",
    "human_mean = counterfactual[counterfactual[\"clip\"] < 19].groupby('clip').mean('rating').rating\n",
    "gpt4v = models[models.COND == \"counterfactual\"][\"GPT-4V\"]\n",
    "\n",
    "# Sort ratings from low high (kind of like a confidence rating)\n",
    "human_lowhigh = np.argsort(human_mean.values)\n",
    "gpt4v_lowhigh = np.argsort(gpt4v.values)\n",
    "print(human_lowhigh+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare plot\n",
    "fig, axs = plt.subplots(1, 4, sharey=False, tight_layout=True, figsize=(6,4))  \n",
    "\n",
    "# First column: Humans say it doesnt go in, GPT says it goes in (13)\n",
    "axs[0].imshow(plt.imread(\"eval/images/michotte_edit/clip_13.png\"))\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "# Second column: Humans say it doesnt go in, GPT says it goes in (8)\n",
    "axs[1].imshow(plt.imread(\"eval/images/michotte_edit/clip_8.png\"))\n",
    "axs[1].axis(\"off\")\n",
    "\n",
    "# Third column: Humans say it goes in, GPT says it goes in (17)\n",
    "axs[2].imshow(plt.imread(\"eval/images/michotte_edit/clip_17.png\"))\n",
    "axs[2].axis(\"off\")\n",
    "\n",
    "# Fourth column: Humans say it goes in, GPT says it goes in (18)\n",
    "axs[3].imshow(plt.imread(\"eval/images/michotte_edit/clip_18.png\"))\n",
    "axs[3].axis(\"off\")\n",
    "\n",
    "# Set column titles\n",
    "axs[0].set_title(\"Humans: out\\nGPT4-V: in\", fontweight=\"bold\")\n",
    "axs[1].set_title(\"Humans: out\\nGPT4-V: in\", fontweight=\"bold\")\n",
    "axs[2].set_title(\"Humans: in\\nGPT4-V: in\", fontweight=\"bold\")\n",
    "axs[3].set_title(\"Humans: in\\nGPT4-V: in\", fontweight=\"bold\")\n",
    "plt.savefig(\"figures/03_michotte_examples.pdf\", dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
