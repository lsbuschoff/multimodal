{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyreadr\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau, fisher_exact, ttest_ind, sem, binom\n",
    "from helpers import plot_params\n",
    "from natsort import natsorted, natsort_keygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human data from Gerstenberg\n",
    "intention = pd.read_csv('data/agents_intention.csv')\n",
    "counterfactual = pd.read_csv('data/agents_counterfactual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model data we collected\n",
    "subjects = [\"FUYU\", \"OTTER\", \"ADAPTER\", \"GPT-4V\", \"CLAUDE-3\"]\n",
    "models = pd.read_csv(\"data/AGENTS_RESULTS.csv\").loc[:, :subjects[-1]]\n",
    "models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Analyze match to ground truth for color test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subjects\n",
    "colors = np.zeros(len(subjects))\n",
    "colors_errs = np.zeros(len(subjects))\n",
    "num_seqs = len(models[models.COND == \"Color\"])\n",
    "\n",
    "# Loop through subjects\n",
    "for ind, subject in enumerate(subjects):  \n",
    "\n",
    "        # Absolute distance to ground truth and absolute distance to human average\n",
    "        correct = sum(models[models.COND == \"Color\"][subject] == \"White\")\n",
    "        colors[ind] = (correct / num_seqs)*100\n",
    "        colors_errs[ind] = binom.std(num_seqs, correct / num_seqs)\n",
    "        print(f\"Color, {subject}: {colors[ind]:.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyze visual understanding test: number of boxes in scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ground truth\n",
    "numbers = np.zeros(len(subjects))\n",
    "numbers_errs = np.zeros(len(subjects))\n",
    "gt = [2, 3, 1, 1, 2, 1, 1, 2, \n",
    "      2, 1, 1, 2, 1, 1, 1, 1, \n",
    "      1, 1, 2, 2, 1, 1, 1, 1]\n",
    "num_seqs = len(gt)\n",
    "\n",
    "# Loop through subjects\n",
    "for ind, subject in enumerate(subjects):  \n",
    "\n",
    "        # Absolute distance to ground truth and absolute distance to human average\n",
    "        correct = sum(np.equal(models[models.COND == \"Number\"][subject].values.astype(int), gt))\n",
    "        numbers[ind] = (correct / num_seqs)*100\n",
    "        numbers_errs[ind] = binom.std(num_seqs, correct / num_seqs)\n",
    "        print(f\"Number of boxes, {subject}: {numbers[ind]:.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regression to humans for intention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save human answers next to model answers for all models\n",
    "# Output shape here is: (Number of humans * Number of test images, 3) \n",
    "# With 3 being: Model answers repeated number of humans times, All human answers, Human index\n",
    "num_humans = len(intention.workerid.unique())\n",
    "num_seqs = len(intention.trial.unique()) - 1 # -1 because nan is also counted\n",
    "human_mean = intention.groupby('trial').mean('int')['int'].values\n",
    "\n",
    "for model in (subjects + [\"HUMAN\"]):\n",
    "\n",
    "    # Loop through all human subjects\n",
    "    temp = np.zeros((num_humans*num_seqs, 3))\n",
    "    temp[:] = np.nan\n",
    "    for ind, human in enumerate(intention.workerid.unique()):\n",
    "\n",
    "        # If array for human to human comparison, save human mean. Otherwise, save model answers in first column.\n",
    "        if model == \"HUMAN\":\n",
    "            temp[ind*num_seqs:(ind+1)*num_seqs, 0] = human_mean\n",
    "        else:\n",
    "            temp[ind*num_seqs:(ind+1)*num_seqs, 0] = models[models.COND == \"Intention\"][model].values.astype(float)\n",
    "\n",
    "        # Save individual human answers and individual human id\n",
    "        ind_human = intention[(intention.workerid == human) & (intention.trial.notna())].sort_values(\"trial\", key=natsort_keygen())\n",
    "        temp[(ind_human.trial.values.astype(int) - 1) + (ind*num_seqs), 1] = ind_human.int.values.astype(float)\n",
    "        temp[ind*num_seqs:(ind+1)*num_seqs, 2] = human\n",
    "\n",
    "    print(f\"{model}, {spearmanr(temp[:, 0], temp[:, 1], nan_policy=\"omit\").statistic:.4f}\")\n",
    "\n",
    "    # Save to read in R after\n",
    "    temp = pd.DataFrame(temp, columns=[\"Model\", \"Human\", \"Ind\"])\n",
    "    temp.to_csv(f\"dataframes/05_agents/{model}_intention.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load linear mixed effects regression models from R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "intention_lmer = pd.read_csv('dataframes/05_agents/05_intention_lmer.csv')\n",
    "\n",
    "# Separate coefficient and re-format CI\n",
    "intention_lmer_coef = intention_lmer.iloc[:,0].values\n",
    "intention_lmer_conf = [(intention_lmer.iloc[:,0] - intention_lmer.iloc[:,1]).values,\n",
    "                       (intention_lmer.iloc[:,2] - intention_lmer.iloc[:,0]).values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Counterfactual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save human answers next to model answers for all models\n",
    "# Output shape here is: (Number of humans * Number of test images, 3) \n",
    "# With 3 being: Model answers repeated number of humans times, All human answers, Human index\n",
    "num_humans = len(counterfactual.workerid.unique())\n",
    "num_seqs = len(counterfactual.trial.unique()) - 1 # -1 because nan is also counted\n",
    "human_mean = counterfactual.groupby('trial').mean('cf')['cf'].values\n",
    "\n",
    "for model in (subjects + [\"HUMAN\"]):\n",
    "\n",
    "    # Loop through all human subjects\n",
    "    temp = np.zeros((num_humans*num_seqs, 3))\n",
    "    temp[:] = np.nan\n",
    "    for ind, human in enumerate(counterfactual.workerid.unique()):\n",
    "\n",
    "        # If array for human to human comparison, save human mean. Otherwise, save model answers in first column.\n",
    "        if model == \"HUMAN\":\n",
    "            temp[ind*num_seqs:(ind+1)*num_seqs, 0] = human_mean\n",
    "        else:\n",
    "            temp[ind*num_seqs:(ind+1)*num_seqs, 0] = models[models.COND == \"Counterfactual\"][model].values.astype(float)\n",
    "\n",
    "        # Save individual human answers and individual human id\n",
    "        ind_human = counterfactual[(counterfactual.workerid == human) & (counterfactual.trial.notna())].sort_values(\"trial\", key=natsort_keygen())\n",
    "        temp[(ind_human.trial.values.astype(int) - 1) + (ind*num_seqs), 1] = ind_human.cf.values.astype(float)\n",
    "        temp[ind*num_seqs:(ind+1)*num_seqs, 2] = human\n",
    "\n",
    "    print(f\"{model}, {spearmanr(temp[:, 0], temp[:, 1], nan_policy=\"omit\").statistic:.4f}\")\n",
    "\n",
    "    # Save to read in R after\n",
    "    temp = pd.DataFrame(temp, columns=[\"Model\", \"Human\", \"Ind\"])\n",
    "    temp.to_csv(f\"dataframes/05_agents/{model}_counterfactual.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load linear mixed effects regression models from R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "counterfactual_lmer = pd.read_csv('dataframes/05_agents/05_counterfactual_lmer.csv')\n",
    "\n",
    "# Separate coefficient and re-format CI\n",
    "counterfactual_lmer_coef = counterfactual_lmer.iloc[:,0].values\n",
    "counterfactual_lmer_conf = counterfactual_lmer.iloc[:,1:].values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make main plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init plotting params\n",
    "model_colors = plot_params()\n",
    "\n",
    "# Prepare plot\n",
    "num_subjects = len(subjects)\n",
    "fig, axs = plt.subplots(1, 4, figsize=(8,3), sharey=False, tight_layout=True)  \n",
    "\n",
    "# Plot A\n",
    "axs[0].bar(np.arange(num_subjects), colors, color=model_colors)\n",
    "axs[0].errorbar(np.arange(num_subjects), colors, yerr=colors_errs, ls=\"none\")\n",
    "axs[0].set_title(\"Background color\\n\")\n",
    "axs[0].set_title(\"A\\n\", fontweight='bold', loc='left', fontsize='medium')\n",
    "axs[0].set_ylabel(\"Percentage correct\")\n",
    "axs[0].set_ylim(0, 100)\n",
    "#axs[0].axhline(y=random_dists[0])\n",
    "axs[0].set_xticks(np.arange(num_subjects))\n",
    "\n",
    "# Plot B\n",
    "axs[1].bar(np.arange(num_subjects), numbers, color=model_colors)\n",
    "axs[1].errorbar(np.arange(num_subjects), numbers, yerr=numbers_errs, ls=\"none\")\n",
    "axs[1].set_title(\"Number of boxes\\n\")\n",
    "axs[1].set_title(\"B\\n\", fontweight='bold', loc='left', fontsize='medium')\n",
    "axs[1].set_ylabel(\"Percentage correct\")\n",
    "axs[1].set_ylim(0, 100)\n",
    "axs[1].set_xticks(np.arange(num_subjects))\n",
    "\n",
    "# Plot C\n",
    "axs[2].bar(np.arange(num_subjects+1), intention_lmer_coef, color=model_colors)\n",
    "axs[2].errorbar(np.arange(num_subjects+1), intention_lmer_coef, yerr=intention_lmer_conf, ls=\"none\")\n",
    "axs[2].set_title(\"Intention\\n\", loc=\"center\")\n",
    "axs[2].set_title(\"C\\n\", fontweight='bold', loc='left', fontsize='medium')\n",
    "axs[2].set_ylabel(\"Regression to humans\")\n",
    "axs[2].set_ylim(-1, 1)\n",
    "axs[2].set_xticks(np.arange(num_subjects+1))\n",
    "\n",
    "# Plot D\n",
    "axs[3].bar(np.arange(num_subjects+1), counterfactual_lmer_coef, color=model_colors)\n",
    "axs[3].errorbar(np.arange(num_subjects+1), counterfactual_lmer_coef, yerr=counterfactual_lmer_conf, ls=\"none\")\n",
    "axs[3].set_title(\"Counterfactual\\n\", loc=\"center\")\n",
    "axs[3].set_title(\"D\\n\", fontweight='bold', loc='left', fontsize='medium')\n",
    "axs[3].set_ylabel(\"Regression to humans\")\n",
    "axs[3].set_ylim(-1, 1)\n",
    "axs[3].set_xticks(np.arange(num_subjects+1))\n",
    "\n",
    "# Despine and set x tick labels afterwards\n",
    "sns.despine(offset=5, trim=True)\n",
    "axs[0].set_xticklabels(subjects, rotation=90)\n",
    "axs[1].set_xticklabels(subjects, rotation=90)\n",
    "axs[2].set_xticklabels(subjects + [\"Humans\"], rotation=90)\n",
    "axs[3].set_xticklabels(subjects + [\"Humans\"], rotation=90)\n",
    "plt.savefig(\"figures/05_agents_main.pdf\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional: example plot for counterfactual condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_mean = counterfactual.groupby('trial').mean('cf')['cf'].values\n",
    "gpt4v = models[models.COND == \"Counterfactual\"][\"GPT-4V\"].values.astype(float)\n",
    "\n",
    "# Sort ratings from low high (kind of like a confidence rating)\n",
    "human_lowhigh = np.argsort(human_mean)\n",
    "gpt4v_lowhigh = np.argsort(gpt4v)\n",
    "print(human_lowhigh+1)\n",
    "print(gpt4v_lowhigh+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare plot\n",
    "fig, axs = plt.subplots(1, 4, sharey=False, tight_layout=True, figsize=(6,4))  \n",
    "\n",
    "# First column: Humans: no success, GPT4V: success (18)\n",
    "axs[0].imshow(plt.imread(\"eval/images/agents/18.png\"))\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "# Second column: Humans: no success, GPT4V: no success (9)\n",
    "axs[1].imshow(plt.imread(\"eval/images/agents/9.png\"))\n",
    "axs[1].axis(\"off\")\n",
    "\n",
    "# Third column: Humans: success, GPT4V: success (13)\n",
    "axs[2].imshow(plt.imread(\"eval/images/agents/13.png\"))\n",
    "axs[2].axis(\"off\")\n",
    "\n",
    "# Fourth column: Humans: success, GPT4V: no success (14)\n",
    "axs[3].imshow(plt.imread(\"eval/images/agents/14.png\"))\n",
    "axs[3].axis(\"off\")\n",
    "\n",
    "# Set column titles\n",
    "axs[0].set_title(\"Humans: no success\\nGPT4-V: success\", fontweight=\"bold\")\n",
    "axs[1].set_title(\"Humans: no success\\nGPT4-V: no success\", fontweight=\"bold\")\n",
    "axs[2].set_title(\"Humans: success\\nGPT4-V: success\", fontweight=\"bold\")\n",
    "axs[3].set_title(\"Humans: success\\nGPT4-V: no success\", fontweight=\"bold\")\n",
    "plt.savefig(\"figures/05_agents_examples.pdf\", dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
